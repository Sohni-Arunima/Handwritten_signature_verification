{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkmqdynzBTql",
    "outputId": "8cb9a9aa-3a8d-426b-f24a-e91f998c4039"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install torch torchvision pillow matplotlib scikit-learn tqdm kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uu5pc-Z3DVWG",
    "outputId": "b07a105c-5a54-413e-ae41-51d3a7aea2a6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# -------------------------------\n",
    "# Enter Kaggle credentials\n",
    "# -------------------------------\n",
    "kaggle_username = \"sohniarunimamaroju\"\n",
    "kaggle_key = \"KGAT_dbd83512dacb55b3812965e2531602bf\"\n",
    "\n",
    "# Create .kaggle folder if not exists\n",
    "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "\n",
    "# Write kaggle.json\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
    "    json.dump({\"username\": kaggle_username, \"key\": kaggle_key}, f)\n",
    "\n",
    "# Set permissions\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "!mkdir -p data/cedar/signatures\n",
    "!kaggle datasets download -d matteocarnebella/cedar-signatures -q\n",
    "!unzip -oq cedar-signatures.zip -d data/cedar/signatures\n",
    "\n",
    "# List files to confirm\n",
    "!ls data/cedar/signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpX2LKLjDbrn",
    "outputId": "af096846-5dcc-4d06-866c-594f0fdf8ac5"
   },
   "outputs": [],
   "source": [
    "import os, glob, re, random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "base_dir = \"/content/data/cedar/signatures/signatures\"\n",
    "if not os.path.exists(base_dir):\n",
    "    raise FileNotFoundError(f\"Folder not found: {base_dir}\")\n",
    "\n",
    "# Helper functions\n",
    "def get_writer_id(fname):\n",
    "    nums = re.findall(r'\\d+', fname)\n",
    "    return nums[0] if nums else \"unknown\"\n",
    "\n",
    "def is_forged(fname):\n",
    "    fn = fname.lower()\n",
    "    return (\"forg\" in fn) or (\"fgr\" in fn)\n",
    "\n",
    "# Collect all images\n",
    "images = []\n",
    "for signer_folder in sorted(os.listdir(base_dir)):\n",
    "    signer_path = os.path.join(base_dir, signer_folder)\n",
    "    if not os.path.isdir(signer_path):\n",
    "        continue\n",
    "    files = sorted(glob.glob(os.path.join(signer_path, \"*.*\")))\n",
    "    for f in files:\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            fname = os.path.basename(f)\n",
    "            m = re.findall(r'\\d+', signer_folder)\n",
    "            writer = m[0] if m else get_writer_id(fname)\n",
    "            forged = is_forged(fname) or (\"forged\" in signer_folder.lower())\n",
    "            images.append({\n",
    "                \"img_path\": f,\n",
    "                \"writer\": writer,\n",
    "                \"fname\": fname,\n",
    "                \"forged\": forged\n",
    "            })\n",
    "\n",
    "print(f\"✅ Total images: {len(images)}, Unique writers: {len(set([x['writer'] for x in images]))}\")\n",
    "print(\"Sample counts per writer:\", Counter([x['writer'] for x in images]).most_common(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0id1fU8xDhbI",
    "outputId": "2228583e-b9c2-4e95-b8bc-0f6377db0baf"
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Transforms (3 channels for MobileNet)\n",
    "# ------------------------------\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((128,128)),\n",
    "    T.RandomRotation(10),\n",
    "    T.RandomAffine(degrees=0, translate=(0.05,0.05), scale=(0.95,1.05), shear=5),\n",
    "    T.Grayscale(num_output_channels=3),  # convert 1 channel -> 3\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((128,128)),\n",
    "    T.Grayscale(num_output_channels=3),  # convert 1 channel -> 3\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "print(\"✅ Transforms ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rOc6rYbDkms"
   },
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, images_list, transform=None, same_prob=0.5):\n",
    "        self.images = images_list\n",
    "        self.transform = transform\n",
    "        self.same_prob = same_prob\n",
    "        self.by_writer = {}\n",
    "        for idx, rec in enumerate(self.images):\n",
    "            self.by_writer.setdefault(rec[\"writer\"], []).append(idx)\n",
    "        self.writers = list(self.by_writer.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a_idx = idx\n",
    "        a = self.images[a_idx]\n",
    "        if random.random() < self.same_prob and len(self.by_writer[a[\"writer\"]]) > 1:\n",
    "            choices = [i for i in self.by_writer[a[\"writer\"]] if i != a_idx]\n",
    "            b_idx = random.choice(choices); label = 1.0\n",
    "        else:\n",
    "            other_writer = random.choice([w for w in self.writers if w != a[\"writer\"]])\n",
    "            b_idx = random.choice(self.by_writer[other_writer]); label = 0.0\n",
    "        b = self.images[b_idx]\n",
    "\n",
    "        img1 = Image.open(a[\"img_path\"]).convert(\"L\")\n",
    "        img2 = Image.open(b[\"img_path\"]).convert(\"L\")\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy5Yl3YGDnFZ",
    "outputId": "3adc8dd5-748d-4770-bb0f-e5eec1e71384"
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Split by writers\n",
    "# ------------------------------\n",
    "writer_list = sorted(list(set([x['writer'] for x in images])))\n",
    "train_w, val_w = train_test_split(writer_list, test_size=0.2, random_state=42)\n",
    "\n",
    "train_imgs = [x for x in images if x['writer'] in train_w]\n",
    "val_imgs   = [x for x in images if x['writer'] in val_w]\n",
    "\n",
    "train_ds = SiameseDataset(train_imgs, transform=train_transform)\n",
    "val_ds   = SiameseDataset(val_imgs, transform=val_transform)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"✅ Done. Writers: {len(writer_list)} | Train pairs: {len(train_ds)} | Val pairs: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SHg9lvOrEBJp",
    "outputId": "e9f14489-c006-4dcb-b081-e8558189ba51"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# ------------------------------\n",
    "# Embedding Network\n",
    "# ------------------------------\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super().__init__()\n",
    "        base = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        base.classifier = nn.Identity()\n",
    "        self.backbone = base\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        e = self.head(f)\n",
    "        return F.normalize(e, p=2, dim=1)  # L2 normalize\n",
    "\n",
    "# ------------------------------\n",
    "# Siamese Network\n",
    "# ------------------------------\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = EmbeddingNet()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        e1 = self.embedding(x1)\n",
    "        e2 = self.embedding(x2)\n",
    "        return e1, e2\n",
    "\n",
    "# ------------------------------\n",
    "# Contrastive Loss\n",
    "# ------------------------------\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, e1, e2, label):\n",
    "        dist = F.pairwise_distance(e1, e2)\n",
    "        loss = label * dist**2 + (1 - label) * F.relu(self.margin - dist)**2\n",
    "        return loss.mean()\n",
    "\n",
    "print(\"✅ Siamese Network and Contrastive Loss ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEHPuMM1EDmG",
    "outputId": "18cfcfbc-03ea-41b1-dee1-7ce997e5bfd3"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ------------------------------\n",
    "# Training function\n",
    "# ------------------------------\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for x1, x2, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        e1, e2 = model(x1, x2)\n",
    "        loss = criterion(e1, e2, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x1.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "# ------------------------------\n",
    "# Validation function\n",
    "# ------------------------------\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    labels = []\n",
    "    dists = []\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "            e1, e2 = model(x1, x2)\n",
    "            loss = criterion(e1, e2, y)\n",
    "            running_loss += loss.item() * x1.size(0)\n",
    "            labels.extend(y.cpu().numpy())\n",
    "            dists.extend(F.pairwise_distance(e1, e2).cpu().numpy())\n",
    "    auc = roc_auc_score(labels, -np.array(dists))  # smaller dist = positive\n",
    "    return running_loss / len(loader.dataset), auc\n",
    "\n",
    "print(\"✅ Training & validation functions ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3JSAzoVEHAv",
    "outputId": "8e871896-4198-4dbd-8937-adb2c90c8225"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model = SiameseNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = ContrastiveLoss(margin=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "81aff3115fd44527be17c4afda8fce70",
      "59a50bfc3c1a4b88976ddc6c36060373",
      "07e5bac668a949028cf36e50c32d5853",
      "5a0079ec688d43a380e262abcdc6dd8e",
      "187b3742248e4928815b299c0187f631",
      "64c8b39df1e74894a7ca43ba3dfb8480",
      "49266f7a9a89403db01deb448babc850",
      "132f0a4e6b7b47d3aefce8e3243921e7",
      "65e84b67178f4d7891194cf3800b590f",
      "1664e64e9a934f2d8f3fdc1ee5c17115",
      "3e0860ac096b4d9290b772517cdfb574",
      "857b6623f14c488dba678dfd32183368",
      "ee7057ff9fe64faab68270ee2d5f781c",
      "d4c39139fd5443958f4a1251a4ae27cf",
      "c187677973b943d0856721f37e4bac8e",
      "4c17a87e5890440d93cad25a646c7f28",
      "038b7486d0d4426bb7fc99d0606e1c7a",
      "71fdecc986114a55bc722e71d628e454",
      "60f98393846440e282e5df7c99b162af",
      "4137b687086d42e383f5aff37b07244c",
      "a363d5de9f754eeca2362f249351c9fe",
      "c9a3a5a266ab4e3f8213322c38afff28",
      "82e308ac7b8649259b5aa5b6f2c3e6a4",
      "f03cdc55b9e84301aa8d6f86ce058576",
      "1a883a7729a646799fd0626960aad447",
      "19a965a2468441659f43fe2f2e81b77f",
      "6fdc41cf6f734ebfa14d950d3d346404",
      "3c9c4d27f45245a9903674513d048267",
      "5bbf70651c6e4452b885542a5dbacd47",
      "4f07a2cf76ed4d358df2a08ec9f90085",
      "23a7d5934b234411b4777db89cd32022",
      "efb043aba0cc4ba6bd00e5e208fbbc86",
      "f161b6d254b24a0b8d099d2d49cbfcdf",
      "02929c39e6e8464b9835d85bb5a82cc6",
      "e5500220decd442c98db718f1d995713",
      "406c6c5c16bc443ba177c7bbffacaa0b",
      "c9407519bbfa4a648fad25953aa4dadf",
      "60ef8b5570034a4cb3be579fcdc055ea",
      "1b6ad930285a4642a08c5bcab7252633",
      "2c648a9fae0e44fdb0d9d016fcbe232e",
      "31ae7365037e4d43a75b4b5e231ade61",
      "50afaadc3ef1475783b20df86f16f5ac",
      "2715611f14b44650819bed5eb8eac789",
      "506121182cb943f69bd5dd0baed6f81e",
      "3a7684ba44124da68bf0a50ee04489d2",
      "23ea8c35ff6949b08d77b9a19ac93a6e",
      "0fdcf493f84f4878bb22f5d8adb629a4",
      "d7139d86048149e28fe92d2023936aba",
      "20481a4b9eab4bee93f0d8fe565c5c45",
      "8d3165273cca43d4905645346f7006ea",
      "c259b86217a54b71a79099286e46964d",
      "8adb970bfce6466da03982669b03bc39",
      "af6d7870e9bd4f0dba4933df74a1cfa4",
      "98a070e7e23f48d48bd2974b6d991554",
      "00d58ddd524c4d94803967c9b4a38ae4",
      "ec22c22a89484435a940e59b3c81310f",
      "d9cd19e7eb3b43c797735d43c61d8c71",
      "d038aea67f594bba8bba409f09e8bdf4",
      "dd6ed6f2c61244d8b5379cda55148cf2",
      "753796addb9b43a78950398500970bc3",
      "692052cc671744aca1749744387bb206",
      "e22933b6cd28437f8a855f55142c9093",
      "00ff96c7ea004f049e927c56341c7299",
      "d5d50114cbad4a3d8800f2ac50cb3a6a",
      "db00b7e427e04a968c8123ffd88adee8",
      "12864d96ed6b4ec79eaed8c0aada9daa",
      "4b208d08e1514f0f8c229337f9cc6de9",
      "62b37c79b8af423488e85d599e837a9f",
      "7586488c1cc04d538dd638b9fa90caee",
      "482d743de19b4f2b9b4409f5566be596",
      "1d89d840af484712be4c7267d88c0832",
      "f96c40ca6edc4a96928f00905b30e1b6",
      "8d61c5d6cec74458b4cbc93581852e26",
      "583e6b1ae4804bcd849dce69a4211791",
      "768a2e1078eb43c48089ed993b9268af",
      "7f111911b91f45e28a4efd36e01e0a3b",
      "11de43a18eb64fb68c7c681f4b94e78b",
      "fadfbdfd3ab5498cb41c9730215a1979",
      "6dd4921a735645a0b65023b762018585",
      "da05cebacd0446d7a9f78d7c44ffad0a",
      "1b5cee42ccbc4ba48390de0fc5bbed5f",
      "ad5ef285d0b4423c93e427b1834e833d",
      "c5f6b0daa79e46acabcd5db00773963d",
      "e8a04127fceb4101940ff4f5d63aed0b",
      "5811fc82125a4719b75ba609e323f3e0",
      "05a1846113ac4e10bc8494d7eccf3108",
      "a0fbcaf0007e44068e25c782c71dabee",
      "945e702792ee4d6ea2e5e61f034e7aeb",
      "68e63f98ffb548db9db0545bf28c49a8",
      "c9c6b03b391743c390f94e935c9454b3",
      "fcf179cbdf9146979fdd5f41bc61d10c",
      "f999d53ec78645b6b2f2b7f60a9c70aa",
      "682787bea2ed4af887c6773d06055bff",
      "80964930b6734f86a1bfb8265d7f4059",
      "80e9fc44e64d49298916cabe048603e4",
      "8847c400a801400babc04da1fecf6bac",
      "e91a156ad7c642cdb6d562e1c475f11a",
      "6a871c40831a46e5b4727680d5875ea6",
      "6e2d9a0752a94ed9811461494921fdc3",
      "0a6b7c86422d4225951616434840d6b1",
      "cf6cbf22d51c4aa881aefc1ff2ff414b",
      "7fca74c6bc3c46718450a6cf0f328dac",
      "9cb2ee1d591544af9f7bf7655b87ae17",
      "95c5647416e74901a3133246f04169dd",
      "8c440b31b1284abaa57600febfee4a7a",
      "ccc1f7b5702042d6b3f1b7e88aa06bc5",
      "259ac6d7ddc54bfb9419ab076a931a77",
      "a6c8007cab37435cbdd9e489e268a572",
      "510a6708da4f467ea6bf8f0e39e598fa",
      "c5fab77a6e664ee7a5a062b6dd05a4a6",
      "24dd4f947ae844988a740a463465de03",
      "f58a1e192a724b26b57152ebd0d45ce4",
      "17772be445cf40b89de7983057426262",
      "621624cea0c24e86a93d41a607996b3d",
      "b3a70bbd9cf54de5af2bf44ee78cb941",
      "94df4f331c314e779934e659f3b9bb82",
      "461cfe87647f4729ad5b3e33d70b4d20",
      "26fc5265bd5745158d88948146774397",
      "aadf977bcd0740518bbd484b62f9d80f",
      "80a15b8f463a43c2958239090218f0f7",
      "ca3dff2f3e5e4125a76ef01f4d074d2b",
      "9050319192eb4abea77ccfa39b059b5a",
      "9563a281c35545fc88d2ec942b99d15f",
      "7d00a083b96443d3a81ff06699a7cdcf",
      "16d2c62838dc44c891578a492ef5cc1d",
      "ea49e5f6198046be9af44231fcf2b949",
      "e7c5a4c0b3de4180b82b2732ad535aa8",
      "107cf47fd9f04aba943ba5f5e1622a50",
      "f11a1ea14354475098bbf80287270ea5",
      "09f6d86e2ac344dc9b1ab0aaeb26afc6",
      "e43eebcc97ba46f6ac97f245d6c69e16",
      "a2f54232ab6c4a0c99788479acd922bf",
      "41980c11a29e4e82b68407be170a1872",
      "1b153d5c481f47bb9f5518f64447474d",
      "de699f60b861434c849d88bdee6089ba",
      "f7e4880a6bdb47a49b4790ffb2aebe52",
      "4aeb15843cf24358b8d23d7c5d2538f9",
      "e517f50159fb4c0798b01af830f14a08",
      "8194d8d3dab348f4baa242b7c1aeb37c",
      "2f2009a5a37f42c795889d7aa4d84f6b",
      "e2ff47ca97be4ae2bf997fdaa32e7e4e",
      "15d69b9b865a40bb8e4f4d74eb9d8f51",
      "76f0e5741a284ca5b8557b0d1b3dd373",
      "7013b7ba54c2467391985943d7613f29",
      "105541ded9c44f41bba53fee38a85342",
      "6898c64fb8364207b032fd9380f0470f",
      "50e75173a2984a318a275e52eaa9eb9e",
      "fac30462fcba43c980e10551a2251bea",
      "5ad601331c3e48c8aa3f2f02fd042363",
      "b4e8fd466b8e4be490c19637ba55e37b",
      "7ecf95e6b0dc41be8b65ab829d129f69",
      "6eee9d73501c426b8369c600a1813e81",
      "f00960f6ce074b31a3e6eeb917f07781",
      "6656f74a856f4bf7b29102187c80e32f",
      "9edde5216432441f9d81390fa6743d2f",
      "6cdd29fe74514ea5a4f9b6c2d01ab6d3",
      "9d20059720bc4d91bb234073458c9421",
      "fa5bbb42d33747799bc7de7f82953140",
      "2f412afe727948b79ebd46da3fc082db",
      "652bb965aa084a51bd6c68e734f93106",
      "3a2122a2c2c440d38890cdf1faaba0bf",
      "8c64aa77a07c4b9ab3c484e628ca7ae7",
      "73c712732e984f2097e9855e0231db65",
      "3874c9e8a16e429588bd1ec6bc9c702c",
      "4f261e95f4764831b6674c454e81ea55",
      "2d17589f0ca64db8bf2ce721e4196822",
      "ebe45f5680b14386a231b4a89acc8d62",
      "c8f131870e5441169047262c6bba514d",
      "4990705a50df4de8b8757f9ff28ef578",
      "bd991e98c81c4119bc3d3dd0422695f1",
      "bd5e5387aec44d08b51e2f972d88e255",
      "5bf7bf26d8ea4feeaca3abf0e68e6a60",
      "f70fbfb365b94bf0a78ba3baf47a4dde",
      "e6160908e2bc46ca9df3de136d031aeb",
      "c53c88c1a712469b8013923537573e19",
      "0d0daae2330c4b058a5ac015bdf2797e",
      "2ab482c8d9224c9695015eb2acd74177",
      "c0237d8797cd4f8ebdf59645dbf3e4f8",
      "791bf222f5aa466a965ea6eed792cb1c",
      "4b59b3d11f484a28b04e26961cb40f80",
      "a069ba6537c04a009ccea9d49adfbc95",
      "6335a226e65643149683589ac841da6c",
      "2e1e44787b39479fa3579ce5443590ee",
      "90515f4b82ec45dba79d75b8f9a22d5e",
      "ab704437709340ebaa83b1b95b236e74",
      "0dcbb95179254f0ebf7d6b9f481624aa",
      "4e7b947207aa41aebc1c66a7d879160b",
      "94e2b0c9cb924f9f99eb476bdac96868",
      "f942ed953bd4455f9051178a2d9e5725",
      "ae72d182844340128f8de0ec83f9dc9f",
      "aada72b866204e56af577ca2a69477ee",
      "ca18a6a3df6748c69bcf503f318630b0",
      "de345fcf64e642b08661b867709b7443",
      "7ba4c94398004a11bf243a4fb8a6648a",
      "627a627aaf6e4b6ebf65a108803a0bf8",
      "da5d8263de554d88bdec703ce65150aa",
      "c8e414f4e1bd417ea5bf416de1115ef5",
      "ea3cc2be05cf4cc7b509e8fe80dec7d4",
      "455b891d2e0641d5bb79524bd8bf30dc",
      "07e7ad9b96c34510a94c64c5e7c42620",
      "6988dbe31e5243d88dc05486b3f06022",
      "b5880399dd9248f7bb364a3a7d6e03d1",
      "f8b9bd8064704d4885645c089ec44636",
      "102b0db745bc4eccbad80c7194fcb48c",
      "89368822f042492ba437aa1bbbd7ea08",
      "e984d725334248b8bd9956819da63d5d",
      "fcfc54eb8d384e5aa014c47a7fd339d9",
      "d450a81d5df147fa8c798f534639342f",
      "c9b5e8fc614246189512d12d006ce0a3",
      "7ab7bc56cfe442299b517dbc15445e14",
      "4e33495176cc4531894502217dd298d5",
      "89a0b4b970474f90889f817edee718ab",
      "cb870376dafc460dad7570d2787caf3c",
      "2b19ef3656bd49b1bf4402928166cca2",
      "86d5d82fa1d14a6eaacbaec0a4b4a383",
      "a786209b99134bd7b2f4b4ff61d0ae10",
      "c58ad7592d184740b04e2607e0536297",
      "1ca40ea0000a42409ae5aea2838aaa20",
      "ebf1f9ad89d74424ba7208ac79e123ff",
      "c743bae2c08740d8be09b54a91967b4f",
      "e08b521be330461db4aab4878ab0f086",
      "94257688a4504c039531a9c3109449c3",
      "e88ca03ef4464dd2af47ddcace349b7d",
      "f7fc4cb08e9944979184eeb375f9e7f2",
      "10e61dd4aea94c57b421ee2481cb7265",
      "33d368f155fd4d7f894badbbe73fa27b",
      "60f7fe54929c4c419412b9b8a6f725e5",
      "d8a9763734484cb8a391b174c178e755",
      "cd468c346b2444d9bdb1099076481512",
      "a59ce077b2f549f2b9c522edf0f2d01b",
      "6b88d902612e466586f51a7f1152e858",
      "5f03963110f2471fbe543635e7a02c3c",
      "bdfa5a745e8b41f3b27b55ef8689f852",
      "e5115c095b9d4138a858db6381caae1b",
      "5ed889ce389244fb89b36181d36e5ff5",
      "c9c1a75fbaf641879fbf2f59e26646f5",
      "54f39fe5741a4bf3afa58aea5bbaf5d4",
      "4a80c662529a4370bf71240ee7020010",
      "3e75e5a8b15e448995045b54d2118bcf",
      "0521ab77d2fd43f5a1fd9a2acf4c2be4",
      "d516ec6ba0bf47e981bcaa230d353f42",
      "6bdfb4941a704012af37abbab0404013",
      "3f5aa887f5b34e478a53855bad4bb7ac",
      "e247b55e4718434793d1eb3e85060bd1",
      "6645852a057b4b5d82950cfb2d2d8a32",
      "bb78bebee73b4aec8169a295c1454768",
      "ac2c10ced8bc4eb5a29e917585411200",
      "5950c490f38a44ffb78308a224ab2dc5",
      "4767f9a106284f11b4eaf0a9a0be664f",
      "0e22b5cc978441ce860114ec2f312d6d",
      "48ce17b45ed24f36a24a48dfbe754b4e",
      "2e8395de9463469b93847256887455e2",
      "17f2e7a2ecc04c6bad2d138a184c7d82",
      "70395129d82a4a48a9b594e291565472",
      "5eddeac230424e90ae9bd8b6eb8185b3",
      "32fe0454c7484c2ab0292dfd49b08d4a",
      "8dfb34c51bed44588b7206d77a603da0",
      "f1a2fe29b285419ea2e9b1539faef218",
      "2f4ec4a9961142749d449eb63cc927dd",
      "3717d82b94354b8ab700bc277d2faebb",
      "9751b196327541cfbf54cce52f242c01",
      "36a0fdeaaa2a4079b70c649d52558074",
      "cedfba09ae434185b5a5f281dfbd05b7",
      "a81e56bf613145a48fb84494aabec83f",
      "d9d7964fde2645c589c9c6f0d89e9efe",
      "8a8d09da5c5249ecaba6e7b825f91fb1",
      "eaacef0868cb46cb8251299ae8ac1444",
      "1f8268cc0cf74b78b55bc56b76239a80",
      "5ab903405e9a4474a879a61fb3dc4627",
      "fe64c6fbd052428c81bc48f983d99cba",
      "d354eebd1b5f4b97bfaac7e400876745",
      "ddee741ab7b046f9a305ed3999953374",
      "b6d868e7fda6448486f28208a351312b",
      "ed50b7b540c44066a3d9cc2b99b13d4f",
      "c54897c557a6402fb06189103b0ccba3",
      "e516b68786a742138b05afe109e2830e",
      "3f0f55785d464547b9540eee33540d49",
      "f168b21daa03452ebfd59df1be0b25d4",
      "fc70afb966d943369f7764dd137c9522",
      "ac1e5fc9fa6f4d0ba8023a4d84a82fc5",
      "b25231cca523460eb5c828346944e6a7",
      "e48b2a4e1e0a415fb513c271e3e90ae5",
      "3ce6fe470fb44923800c30c9be653f2e",
      "02489ef586264844a38f600477bb09d8",
      "60e5de6837cb478f809582f9be0afc25",
      "4cf4b4e002ed42e0b4887e0295ecb85a",
      "84f400a8dc614542ab0f886631c0bd27",
      "7ecf0b2abb6d4edc9ee87c3f19088107",
      "527f255d88a8434a9dbc624fdc6fc072",
      "54dea41ed19945188f6949c46216193d",
      "c775cd1f77bf4af283851b1deabc98ba",
      "9486788062b44c34985b9e17ef72dc7e",
      "b81aa425d32e4ee79e25149878b681e6",
      "52db6ac25a734681a058885fb82d8186",
      "82dcec42399a4546a54570e517dd6388",
      "7a66ae9dc0894405995a957a2d2b7351",
      "bbffa8da71e145789d0b337349a003e8",
      "a558711c2dcb479e9ebfd7b1c78c22f1",
      "0bab9a1a0ca849138425cebcd4ae0b18",
      "c4e8b7796d424a1c87292260b68a9ee9",
      "03a48fd1365645dc9bc80a5ac3e02afa",
      "e1c179720fef4975be4516c9029a56f0",
      "f5df988aec4d4acab9dfaa69d7121622",
      "7b51e63407b14c5899e008baea38f093",
      "01c3aafed72947b881ae71ebd314c471",
      "cfcf5787df094231aa94dfdad5dcfacd",
      "fba6375a81ff49248f2a8a2005f68dc4",
      "15775e345dd84c109d7cbed14fbb7281",
      "210a2e54c1a54ab280b7619bc4f397c3",
      "a6bdf2b39fcd4f11894d1497894df291",
      "736c3a61ceac42e4a49240f99de82665",
      "5b40c953941c456aaec7f642385d7210",
      "196070e431cf49ccb24d793669328e63",
      "bfc28d8b7df54da9a232465195f8613d",
      "1565a58dc0d14aea81e27fa8a4e49a92",
      "a101057189f94aeca1b8ab624d0dc80c",
      "16d09dce52d7487ca401d70c2c020f31",
      "2af144437f594a8596b08442e7d119ff",
      "a357f33893bf4fe4a4aa4de69c0a7449",
      "1554e64bf7c9427c9b56e3537e8c18c1",
      "570919b1a95d4baeb077f211696302ae",
      "b912fd76b52d46e99289d3cc225ac35f",
      "3ca7e042fe914e6d8c71e9ca712a2d8b",
      "4435613e308842489522c8d40fe16ec8",
      "8f4332ce28f84d99a001fd8ec12b94bc",
      "c387c9b779ce4e0696016df7c582d76e",
      "2add5e9eabbc4a669b94190811cfd634",
      "409dcbfd26b0403dbc11e3e1509e14a2",
      "b855ba3728d44c89af8715ff881565d9",
      "32a6268591184fd3aabba1fab4eb760b"
     ]
    },
    "id": "4uf8p04bEKEW",
    "outputId": "ae65fa3f-680b-4d79-bac0-d1d4fe5c956c"
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# ADVANCED TRAINING LOOP\n",
    "# ------------------------------\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "epochs = 40\n",
    "initial_lr = 1e-3\n",
    "patience = 7\n",
    "min_delta = 1e-4\n",
    "model_path_best = \"siamese_best_auc.pth\"\n",
    "use_amp = torch.cuda.is_available()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Training device:\", device, \"| AMP:\", use_amp)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "# scheduler: reduce LR when val AUC plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# criterion\n",
    "criterion = ContrastiveLoss(margin=1.2)\n",
    "\n",
    "# AMP scaler\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "best_val_auc = -np.inf\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "train_losses, val_losses, val_aucs = [], [], []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Train\", leave=False)\n",
    "    for x1, x2, y in train_pbar:\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            e1, e2 = model(x1, x2)\n",
    "            loss = criterion(e1, e2, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        b = x1.size(0)\n",
    "        running_loss += loss.item() * b\n",
    "        total += b\n",
    "        train_pbar.set_postfix({\"loss\": f\"{running_loss/total:.4f}\"})\n",
    "    train_loss = running_loss / total if total > 0 else 0.0\n",
    "\n",
    "    # ---- validate ----\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    labels = []\n",
    "    dists = []\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Val\", leave=False)\n",
    "        for x1, x2, y in val_pbar:\n",
    "            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                e1, e2 = model(x1, x2)\n",
    "                loss = criterion(e1, e2, y)\n",
    "            b = x1.size(0)\n",
    "            running_loss += loss.item() * b\n",
    "            total += b\n",
    "            labels.extend(y.cpu().numpy())\n",
    "            dists.extend(F.pairwise_distance(e1, e2).cpu().numpy())\n",
    "            val_pbar.set_postfix({\"loss\": f\"{running_loss/total:.4f}\"})\n",
    "    val_loss = running_loss / total if total > 0 else 0.0\n",
    "\n",
    "    # compute AUC safely\n",
    "    try:\n",
    "        val_auc = roc_auc_score(labels, -np.array(dists))\n",
    "    except Exception:\n",
    "        val_auc = float(\"nan\")\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_aucs.append(val_auc)\n",
    "\n",
    "    # scheduler step uses val_auc (if valid)\n",
    "    if not np.isnan(val_auc):\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "    # checkpoint best\n",
    "    improved = False\n",
    "    if not np.isnan(val_auc) and val_auc > best_val_auc + min_delta:\n",
    "        best_val_auc = val_auc\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_state, model_path_best)\n",
    "        improved = True\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch+1}/{epochs} — train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | val_auc: {val_auc:.4f} | time: {epoch_time:.1f}s {'(improved)' if improved else ''}\")\n",
    "\n",
    "    # early stopping\n",
    "    if no_improve >= patience:\n",
    "        print(f\"Early stopping: no improvement for {patience} epochs. Best val AUC: {best_val_auc:.4f}\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining finished in {total_time/60:.2f} minutes. Best val AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "# load best checkpoint if exists\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(\"Loaded best model from checkpoint:\", model_path_best)\n",
    "else:\n",
    "    print(\"No checkpoint saved (no valid val_auc improvement).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZjpISvBmDdd",
    "outputId": "0fa9cb78-d93b-44d9-e249-30e4cd1c10a4"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# THRESHOLD TUNING & EVAL\n",
    "# =========================\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report, accuracy_score\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# choose model file\n",
    "if os.path.exists(\"siamese_best_auc.pth\"):\n",
    "    ckpt = \"siamese_best_auc.pth\"\n",
    "elif os.path.exists(\"siamese_model.pth\"):\n",
    "    ckpt = \"siamese_model.pth\"\n",
    "else:\n",
    "    raise FileNotFoundError(\"No model checkpoint found. Train or place siamese_best_auc.pth / siamese_model.pth in the working dir.\")\n",
    "\n",
    "print(\"Loading model from:\", ckpt)\n",
    "model.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "model.to(device).eval()\n",
    "\n",
    "# collect val distances and labels\n",
    "all_labels, all_dists = [], []\n",
    "with torch.no_grad():\n",
    "    for x1, x2, y in val_loader:\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        e1, e2 = model(x1, x2)\n",
    "        dist = F.pairwise_distance(e1, e2)\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        all_dists.extend(dist.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_dists = np.array(all_dists)\n",
    "print(f\"Collected {len(all_dists)} validation distances. Labels diff: {np.unique(all_labels, return_counts=True)}\")\n",
    "\n",
    "# compute ROC-AUC\n",
    "try:\n",
    "    auc = roc_auc_score(all_labels, -all_dists)\n",
    "except Exception as e:\n",
    "    auc = float(\"nan\")\n",
    "print(f\"Val ROC-AUC: {auc:.4f}\")\n",
    "\n",
    "# find best threshold by maximizing accuracy (or Youden)\n",
    "ths = np.linspace(all_dists.min(), all_dists.max(), 200)\n",
    "best_acc, best_t = -1, None\n",
    "for t in ths:\n",
    "    preds = (all_dists < t).astype(int)\n",
    "    acc = accuracy_score(all_labels, preds)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_t = t\n",
    "\n",
    "#compute youden index threshold\n",
    "fpr, tpr, roc_th = roc_curve(all_labels, -all_dists)\n",
    "youden_idx = np.argmax(tpr - fpr)\n",
    "youden_t = roc_th[youden_idx]\n",
    "\n",
    "# compute EER\n",
    "fnr = 1 - tpr\n",
    "eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
    "\n",
    "print(f\"Best-acc threshold: {best_t:.4f} | Accuracy at best: {best_acc:.4f}\")\n",
    "print(f\"Youden threshold (from ROC): {youden_t:.4f}\")\n",
    "print(f\"EER ≈ {eer:.4f}\")\n",
    "\n",
    "#confusion matrix at best_t\n",
    "preds_best = (all_dists < best_t).astype(int)\n",
    "cm = confusion_matrix(all_labels, preds_best)\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "print(\"\\nClassification report:\\n\", classification_report(all_labels, preds_best, digits=4))\n",
    "\n",
    "# Save threshold for inference\n",
    "with open(\"best_threshold.txt\", \"w\") as f:\n",
    "    f.write(str(best_t))\n",
    "print(\"Saved best threshold to best_threshold.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "6R_sESsfIolq",
    "outputId": "41d7cc21-f754-430b-8853-e2b416281687"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# Plot Loss and ROC-AUC curves\n",
    "# ------------------------------\n",
    "epochs_range = range(1, len(train_losses)+1)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs_range, train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(epochs_range, val_losses, label='Val Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ROC-AUC curves\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs_range, val_aucs, label='Val ROC-AUC', marker='o', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.title('Validation ROC-AUC')\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2-lf3gTIpx4",
    "outputId": "d04cbd5a-7a6a-41dd-ccd5-3685a45f8edc"
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Save the trained model\n",
    "# ------------------------------\n",
    "model_path = \"siamese_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"✅ Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLszcPmlIsbb",
    "outputId": "b7b46658-cd33-48e3-8dae-4b16ecd1d506"
   },
   "outputs": [],
   "source": [
    "model = SiameseNet().to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "print(\"✅ Model loaded and ready for inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-kpSthKoI8Zp",
    "outputId": "11c71b5f-70fd-4868-a40c-d25d93871336"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "model.eval()  # set to evaluation mode\n",
    "\n",
    "# pick 5 random pairs from validation set\n",
    "for _ in range(5):\n",
    "    x1, x2, label = random.choice(val_ds)\n",
    "    x1, x2 = x1.unsqueeze(0).to(device), x2.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        e1, e2 = model(x1, x2)\n",
    "        dist = F.pairwise_distance(e1, e2)\n",
    "        pred = 1 if dist.item() < 1.0 else 0  # threshold = 1.0\n",
    "\n",
    "    print(f\"True Label: {int(label.item())} | Predicted: {pred} | Distance: {dist.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yCW8XkBrI_2h",
    "outputId": "26194b84-4b76-48fe-c25f-b5b18d0b5566"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "threshold = 1.0  # distance threshold for predicting same/different\n",
    "\n",
    "num_samples = 6  # number of pairs to visualize\n",
    "samples = list(val_loader)[:num_samples]  # get first batch\n",
    "\n",
    "x1_batch, x2_batch, y_batch = samples[0]\n",
    "x1_batch, x2_batch = x1_batch.to(device), x2_batch.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    e1, e2 = model(x1_batch, x2_batch)\n",
    "    dists = F.pairwise_distance(e1, e2).cpu().numpy()\n",
    "    labels = y_batch.numpy()\n",
    "    preds = (dists < threshold).astype(int)\n",
    "\n",
    "# Plot images with predictions\n",
    "plt.figure(figsize=(12, num_samples*2))\n",
    "for i in range(num_samples):\n",
    "    img1 = x1_batch[i].cpu().permute(1,2,0).numpy() * 0.5 + 0.5  # unnormalize\n",
    "    img2 = x2_batch[i].cpu().permute(1,2,0).numpy() * 0.5 + 0.5\n",
    "\n",
    "    plt.subplot(num_samples, 2, 2*i+1)\n",
    "    plt.imshow(img1.squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if i == 0: plt.title(\"Image 1\")\n",
    "\n",
    "    plt.subplot(num_samples, 2, 2*i+2)\n",
    "    plt.imshow(img2.squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if i == 0: plt.title(\"Image 2\")\n",
    "\n",
    "    plt.suptitle(f\"True: {labels[i]} | Pred: {preds[i]} | Dist: {dists[i]:.4f}\", y=0.92-i*0.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "suDWVlGMJFoS",
    "outputId": "75773209-1d3d-4805-bfbc-4894a45c70ec"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FINAL INFERENCE CELL (upload + test)\n",
    "# =========================\n",
    "from google.colab import files\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# determine threshold\n",
    "if os.path.exists(\"best_threshold.txt\"):\n",
    "    with open(\"best_threshold.txt\",\"r\") as f:\n",
    "        best_thresh = float(f.read().strip())\n",
    "else:\n",
    "    best_thresh = 1.0\n",
    "print(\"Using threshold:\", best_thresh)\n",
    "\n",
    "# test transform\n",
    "IMG_SIZE = 160 if (os.path.exists(\"siamese_best_auc.pth\") and '160' in str(train_transform)) else 128\n",
    "\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    T.Grayscale(num_output_channels=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "# reload best model\n",
    "ckpt = \"siamese_best_auc.pth\" if os.path.exists(\"siamese_best_auc.pth\") else \"siamese_model.pth\"\n",
    "model.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "model.to(device).eval()\n",
    "print(\"Loaded model:\", ckpt)\n",
    "\n",
    "# helper\n",
    "def emb_from_path(p):\n",
    "    img = Image.open(p).convert(\"L\")\n",
    "    t = test_transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        e, _ = model(t,t)\n",
    "    return e\n",
    "\n",
    "# upload two images\n",
    "print(\"Upload your reference (genuine) images — at least 1 (press choose files):\")\n",
    "uploaded_refs = files.upload()\n",
    "ref_paths = list(uploaded_refs.keys())\n",
    "print(\"Uploaded references:\", ref_paths)\n",
    "\n",
    "print(\"\\nUpload the test signature to verify:\")\n",
    "uploaded_test = files.upload()\n",
    "test_path = list(uploaded_test.keys())[0]\n",
    "print(\"Test image:\", test_path)\n",
    "\n",
    "# compute distances to all references\n",
    "ref_embs = [emb_from_path(p) for p in ref_paths]\n",
    "test_emb = emb_from_path(test_path)\n",
    "\n",
    "dists = [float(F.pairwise_distance(r, test_emb).item()) for r in ref_embs]\n",
    "print(\"Distances to references:\", dists)\n",
    "avg_dist = sum(dists)/len(dists)\n",
    "print(\"Average distance:\", avg_dist)\n",
    "\n",
    "if avg_dist < best_thresh:\n",
    "    print(\"\\n=> Prediction: ✅ GENUINE signature (avg_dist < threshold)\")\n",
    "else:\n",
    "    print(\"\\n=> Prediction: ❌ FORGED signature (avg_dist >= threshold)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ARtA0K9nn0x",
    "outputId": "6bbc3dfb-b35b-4dec-dcd6-319af5484d26"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "#  IMPROVED: SIGNATURE VERIFICATION\n",
    "# ==========================================\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# -------- PARAMETERS --------\n",
    "\n",
    "IMG_SIZE = 128\n",
    "DEFAULT_THRESHOLD = 1.0   # fallback threshold if best_threshold.txt not found\n",
    "\n",
    "# -------- Test transform  ----------\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.Grayscale(num_output_channels=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "# -------- Load threshold (auto) ----------\n",
    "if os.path.exists(\"best_threshold.txt\"):\n",
    "    try:\n",
    "        with open(\"best_threshold.txt\", \"r\") as f:\n",
    "            best_threshold = float(f.read().strip())\n",
    "        print(f\"Loaded best_threshold.txt -> {best_threshold:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not read best_threshold.txt, using default threshold:\", e)\n",
    "        best_threshold = DEFAULT_THRESHOLD\n",
    "else:\n",
    "    best_threshold = DEFAULT_THRESHOLD\n",
    "    print(\"best_threshold.txt not found — using default threshold:\", best_threshold)\n",
    "\n",
    "# -------- Load & check model ----------\n",
    "if 'model' not in globals():\n",
    "    raise RuntimeError(\"Model object not found. Make sure you have loaded your model (siamese/model) before running this cell.\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model is loaded and in eval mode. Device:\", device)\n",
    "\n",
    "# -------- Helper: compute embedding----------\n",
    "@torch.no_grad()\n",
    "def emb_from_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "    img = Image.open(path).convert(\"L\")\n",
    "    t = test_transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        # model(x,x) returns (e1,e2) so we take e1\n",
    "        e1, e2 = model(t, t)\n",
    "    return e1\n",
    "\n",
    "# -------- Verify Signature ----------\n",
    "def verify_signature_with_refs(model, reference_paths, test_path, device, threshold=None):\n",
    "    if threshold is None:\n",
    "        threshold = best_threshold\n",
    "\n",
    "    # check files\n",
    "    missing = [p for p in reference_paths + [test_path] if not os.path.exists(p)]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(\"These files are missing: \" + \", \".join(missing))\n",
    "\n",
    "    # cache reference embeddings\n",
    "    ref_embs = []\n",
    "    for p in reference_paths:\n",
    "        try:\n",
    "            ref_embs.append(emb_from_path(p))\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error computing embedding for reference {p}: {e}\")\n",
    "\n",
    "    # test embedding\n",
    "    test_emb = emb_from_path(test_path)\n",
    "\n",
    "    # compute distances\n",
    "    distances = [float(F.pairwise_distance(r, test_emb).item()) for r in ref_embs]\n",
    "    avg_dist = sum(distances) / len(distances)\n",
    "\n",
    "    # print results\n",
    "    print(\"\\n--- Signature Verification Result ---\")\n",
    "    print(\"Reference paths:\")\n",
    "    for i, p in enumerate(reference_paths):\n",
    "        print(f\"  Ref {i+1}: {p}\")\n",
    "    print(\"\\nDistances to references:\")\n",
    "    for i, d in enumerate(distances):\n",
    "        print(f\"  Reference {i+1}: {d:.4f}\")\n",
    "    print(f\"\\nAverage Distance: {avg_dist:.4f}\")\n",
    "    print(f\"Threshold used: {threshold:.4f}\")\n",
    "\n",
    "    if avg_dist < threshold:\n",
    "        print(\"\\n=> Prediction: ✅ GENUINE signature\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\n=> Prediction: ❌ FORGED signature\")\n",
    "        return False\n",
    "\n",
    "# -------- Example usage ----------\n",
    "reference_signs = [\n",
    "    \"/content/my1.jpeg\",\n",
    "    \"/content/my1.jpeg\",\n",
    "    \"/content/my1.jpeg\"\n",
    "]\n",
    "\n",
    "\n",
    "test_sign = \"/content/myf.jpeg\"\n",
    "\n",
    "# Run verification\n",
    "try:\n",
    "    ok = verify_signature_with_refs(model, reference_signs, test_sign, device, threshold=None)\n",
    "except Exception as e:\n",
    "    print(\"Error during verification:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
